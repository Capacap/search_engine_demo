{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0e33b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Image search interface - logging initialized\n",
      "INFO: Device: cuda\n",
      "INFO: GPU: NVIDIA GeForce RTX 3060 Ti\n",
      "INFO: VRAM: 8.2 GB\n",
      "INFO: Loading openai/clip-vit-base-patch32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Model loaded successfully\n",
      "INFO: Loading embeddings from ../data/flickr8k_embeddings.h5...\n",
      "INFO: Loaded 40455 embedding pairs\n",
      "INFO: System initialization complete - ready for bidirectional search\n"
     ]
    }
   ],
   "source": [
    "# Model configuration  \n",
    "MODEL_NAME = \"openai/clip-vit-base-patch32\"\n",
    "TEXT_MAX_LENGTH = 77\n",
    "\n",
    "# Data file paths\n",
    "EMBEDDINGS_FILE = \"../data/flickr8k_embeddings.h5\"\n",
    "\n",
    "# Search configuration\n",
    "DEFAULT_TOP_K = 5\n",
    "\n",
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import torch\n",
    "import h5py\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(levelname)s: %(message)s',\n",
    "    stream=sys.stdout,\n",
    "    force=True\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(\"Image search interface - logging initialized\")\n",
    "\n",
    "# Device Detection and Model Loading\n",
    "def get_device():\n",
    "    \"\"\"Check if GPU is available and set up computing device\"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    logger.info(f\"Device: {device}\")\n",
    "    if torch.cuda.is_available():\n",
    "        logger.info(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        logger.info(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    return device\n",
    "\n",
    "def load_model(model_name, device):\n",
    "    \"\"\"Load the CLIP model for processing images and text\"\"\"\n",
    "    logger.info(f\"Loading {model_name}...\")\n",
    "    model = CLIPModel.from_pretrained(model_name).to(device)\n",
    "    processor = CLIPProcessor.from_pretrained(model_name)\n",
    "    logger.info(f\"Model loaded successfully\")\n",
    "    return model, processor\n",
    "\n",
    "# Load embeddings from file\n",
    "def load_embeddings(filepath: str) -> dict:\n",
    "    \"\"\"Load precomputed embeddings from saved file\"\"\"\n",
    "    logger.info(f\"Loading embeddings from {filepath}...\")\n",
    "    \n",
    "    with h5py.File(filepath, 'r') as f:\n",
    "        # Verify required data is present\n",
    "        required_keys = {'image_embeddings', 'text_embeddings', 'image_names', 'captions'}\n",
    "        if not required_keys.issubset(f.keys()):\n",
    "            raise ValueError(f\"Missing required datasets in {filepath}\")\n",
    "        \n",
    "        # Load all the embedding data\n",
    "        embeddings_data = {\n",
    "            'image_embeddings': f['image_embeddings'][:],\n",
    "            'text_embeddings': f['text_embeddings'][:],\n",
    "            'image_names': [name.decode('utf-8') for name in f['image_names'][:]], \n",
    "            'captions': [cap.decode('utf-8') for cap in f['captions'][:]], \n",
    "            'metadata': dict(f.attrs)\n",
    "        }\n",
    "        \n",
    "        # Check that image folder exists\n",
    "        images_path = embeddings_data['metadata'].get('images_path', '')\n",
    "        if not images_path:\n",
    "            raise ValueError(\"Images path not found in metadata\")\n",
    "        \n",
    "        if not os.path.exists(images_path):\n",
    "            raise FileNotFoundError(f\"Images directory not found: {images_path}\")\n",
    "    \n",
    "    logger.info(f\"Loaded {len(embeddings_data['image_names'])} embedding pairs\")\n",
    "    return embeddings_data\n",
    "\n",
    "# Text to image search\n",
    "def search_images_by_text_diverse(\n",
    "    query_text: str, \n",
    "    embeddings_data: dict, \n",
    "    model, \n",
    "    processor, \n",
    "    device, \n",
    "    top_k: int = DEFAULT_TOP_K\n",
    "    ) -> list:\n",
    "    \"\"\"Search for images using text, ensuring each image appears only once\"\"\"\n",
    "    \n",
    "    # Convert query text to embeddings\n",
    "    text_inputs = processor(\n",
    "        text=query_text, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        max_length=TEXT_MAX_LENGTH\n",
    "        ).to(device)\n",
    "    \n",
    "    # Normalize the query embedding for similarity calculation\n",
    "    with torch.no_grad():\n",
    "        query_embedding = model.get_text_features(**text_inputs)\n",
    "        query_embedding = query_embedding.cpu().numpy()\n",
    "        query_norm = query_embedding / np.linalg.norm(query_embedding)\n",
    "    \n",
    "    # Calculate similarity between query and all images\n",
    "    image_embeddings = np.array(embeddings_data['image_embeddings'])\n",
    "    similarities = image_embeddings @ query_norm.T\n",
    "    similarities = similarities.flatten()\n",
    "    \n",
    "    # Find the best caption for each unique image\n",
    "    image_best_matches = {}\n",
    "    for idx, similarity in enumerate(similarities):\n",
    "        image_name = embeddings_data['image_names'][idx]\n",
    "        caption = embeddings_data['captions'][idx]\n",
    "        \n",
    "        if image_name not in image_best_matches or similarity > image_best_matches[image_name]['similarity']:\n",
    "            image_best_matches[image_name] = {\n",
    "                'similarity': similarity,\n",
    "                'caption': caption,\n",
    "                'index': idx\n",
    "            }\n",
    "    \n",
    "    # Sort results by similarity score\n",
    "    unique_results = []\n",
    "    for image_name, match_data in image_best_matches.items():\n",
    "        unique_results.append({\n",
    "            'image_name': image_name,\n",
    "            'caption': match_data['caption'],\n",
    "            'similarity_score': float(match_data['similarity']),\n",
    "            'index': match_data['index']\n",
    "        })\n",
    "    \n",
    "    unique_results.sort(key=lambda x: x['similarity_score'], reverse=True)\n",
    "    \n",
    "    # Add rank numbers to results\n",
    "    for i, result in enumerate(unique_results[:top_k]):\n",
    "        result['rank'] = i + 1\n",
    "    \n",
    "    return unique_results[:top_k]\n",
    "\n",
    "# Image to text search - find similar captions for an image\n",
    "def search_text_by_image(\n",
    "    query_image: Image.Image,\n",
    "    embeddings_data: dict,\n",
    "    model,\n",
    "    processor,\n",
    "    device,\n",
    "    top_k: int = DEFAULT_TOP_K\n",
    ") -> list:\n",
    "    \"\"\"Find text captions that are similar to an uploaded image\"\"\"\n",
    "    \n",
    "    # Convert image to embeddings\n",
    "    image_inputs = processor(\n",
    "        images=query_image,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    ).to(device)\n",
    "    \n",
    "    # Normalize the image embedding for similarity calculation\n",
    "    with torch.no_grad():\n",
    "        query_embedding = model.get_image_features(**image_inputs)\n",
    "        query_embedding = query_embedding.cpu().numpy()\n",
    "        query_norm = query_embedding / np.linalg.norm(query_embedding)\n",
    "    \n",
    "    # Calculate similarity between image and all text captions\n",
    "    text_embeddings = np.array(embeddings_data['text_embeddings'])\n",
    "    similarities = text_embeddings @ query_norm.T\n",
    "    similarities = similarities.flatten()\n",
    "    \n",
    "    # Get the most similar captions\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "    \n",
    "    results = []\n",
    "    for i, idx in enumerate(top_indices):\n",
    "        results.append({\n",
    "            'caption': embeddings_data['captions'][idx],\n",
    "            'image_name': embeddings_data['image_names'][idx], \n",
    "            'similarity_score': float(similarities[idx]),\n",
    "            'rank': i + 1,\n",
    "            'index': int(idx)\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Set up the system\n",
    "device = get_device()\n",
    "model, processor = load_model(MODEL_NAME, device)\n",
    "embeddings_data = load_embeddings(EMBEDDINGS_FILE)\n",
    "images_path = embeddings_data['metadata']['images_path']\n",
    "\n",
    "logger.info(\"System initialization complete - ready for bidirectional search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bada54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Starting image search interface...\n",
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "INFO: HTTP Request: GET http://127.0.0.1:7861/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: HEAD http://127.0.0.1:7861/ \"HTTP/1.1 200 OK\"\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# Gradio interface\n",
    "\n",
    "def format_results_for_gallery(results: list, images_path: str):\n",
    "    \"\"\"Transform search results into Gradio Gallery format\"\"\"\n",
    "    gallery_data = []\n",
    "    result_text = f\"Found {len(results)} semantically similar images:\\n\\n\"\n",
    "    \n",
    "    for result in results:\n",
    "        image_path = os.path.join(images_path, result['image_name'])\n",
    "        if os.path.exists(image_path):\n",
    "            gallery_data.append((image_path, f\"Rank {result['rank']}: {result['caption'][:100]}...\"))\n",
    "            result_text += f\"**Rank {result['rank']}** (Similarity: {result['similarity_score']:.3f})\\n\"\n",
    "            result_text += f\"Image: `{result['image_name']}`\\n\"\n",
    "            result_text += f\"Caption: _{result['caption']}_\\n\\n\"\n",
    "        else:\n",
    "            logger.warning(f\"Image not found: {image_path}\")\n",
    "    \n",
    "    return gallery_data, result_text\n",
    "\n",
    "def perform_text_to_image_search(query_text: str, num_results: int = 5):\n",
    "    \"\"\"Execute semantic search with proper error handling\"\"\"\n",
    "    if not query_text or query_text.strip() == \"\":\n",
    "        return [], \"Please enter a search query.\"\n",
    "    \n",
    "    try:\n",
    "        logger.info(f\"Searching for: '{query_text}'\")\n",
    "        results = search_images_by_text_diverse(\n",
    "            query_text=query_text.strip(),\n",
    "            embeddings_data=embeddings_data,\n",
    "            model=model,\n",
    "            processor=processor,\n",
    "            device=device,\n",
    "            top_k=num_results\n",
    "        )\n",
    "        \n",
    "        if not results:\n",
    "            return [], f\"No results found for query: '{query_text}'\"\n",
    "        \n",
    "        gallery_data, result_text = format_results_for_gallery(results, images_path)\n",
    "        logger.info(f\"Query completed - {len(results)} results returned\")\n",
    "        return gallery_data, result_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Search failed: {str(e)}\"\n",
    "        logger.error(error_msg)\n",
    "        return [], error_msg\n",
    "\n",
    "def format_text_results(results: list) -> str:\n",
    "    \"\"\"Transform image-to-text results into readable format\"\"\"\n",
    "    if not results:\n",
    "        return \"No similar text descriptions found.\"\n",
    "    \n",
    "    result_text = f\"Found {len(results)} semantically related descriptions:\\n\\n\"\n",
    "    \n",
    "    for result in results:\n",
    "        result_text += f\"**Rank {result['rank']}** (Similarity: {result['similarity_score']:.3f})\\n\"\n",
    "        result_text += f\"Caption: _{result['caption']}_\\n\"\n",
    "        result_text += f\"Source Image: `{result['image_name']}`\\n\\n\"\n",
    "    \n",
    "    return result_text\n",
    "\n",
    "def perform_image_to_text_search(image, num_results: int = 5):\n",
    "    \"\"\"Execute image-to-text search with comprehensive error handling\"\"\"\n",
    "    if image is None:\n",
    "        return \"Please upload an image to search for similar text descriptions.\"\n",
    "    \n",
    "    try:\n",
    "        logger.info(\"Processing uploaded image for text search...\")\n",
    "        \n",
    "        # Search for similar text descriptions\n",
    "        results = search_text_by_image(\n",
    "            query_image=image,\n",
    "            embeddings_data=embeddings_data,\n",
    "            model=model,\n",
    "            processor=processor,\n",
    "            device=device,\n",
    "            top_k=num_results\n",
    "        )\n",
    "        \n",
    "        if not results:\n",
    "            return \"No semantically similar text descriptions found.\"\n",
    "        \n",
    "        formatted_results = format_text_results(results)\n",
    "        logger.info(f\"Image analysis completed - {len(results)} text matches found\")\n",
    "        return formatted_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Image analysis failed: {str(e)}\"\n",
    "        logger.error(error_msg)\n",
    "        return error_msg\n",
    "\n",
    "# Create the web interface\n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Soft(), title=\"Image Search Engine\") as multimodal_demo:\n",
    "    gr.Markdown(\"# üîç Image Search Engine\")\n",
    "    gr.Markdown(\"*Search images using natural language descriptions*\")\n",
    "    gr.Markdown(\"---\")\n",
    "    \n",
    "    with gr.Tabs():\n",
    "        with gr.TabItem(\"üìù Text ‚Üí üñºÔ∏è Images\", elem_id=\"text2image\"):\n",
    "            gr.Markdown(\"### Search for images\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=3):\n",
    "                    text_input = gr.Textbox(\n",
    "                        label=\"Search Query\", \n",
    "                        placeholder=\"Enter your search query (e.g., 'a dog playing in the park')\",\n",
    "                        lines=2\n",
    "                    )\n",
    "                with gr.Column(scale=1):\n",
    "                    num_results = gr.Slider(\n",
    "                        minimum=1, \n",
    "                        maximum=20, \n",
    "                        value=5, \n",
    "                        step=1, \n",
    "                        label=\"Results Count\"\n",
    "                    )\n",
    "            \n",
    "            text_btn = gr.Button(\"üîç Search Images\", variant=\"primary\", size=\"lg\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    image_gallery = gr.Gallery(\n",
    "                        label=\"Search Results\",\n",
    "                        show_label=True,\n",
    "                        elem_id=\"gallery\",\n",
    "                        columns=3,\n",
    "                        rows=2,\n",
    "                        object_fit=\"contain\",\n",
    "                        height=\"auto\"\n",
    "                    )\n",
    "                \n",
    "            with gr.Row():\n",
    "                result_details = gr.Markdown(\n",
    "                    value=\"Enter a search query above to find relevant images.\",\n",
    "                    label=\"Search Results\"\n",
    "                )\n",
    "            \n",
    "        with gr.TabItem(\"üñºÔ∏è Image ‚Üí üìù Text\", elem_id=\"image2text\"):\n",
    "            gr.Markdown(\"### Find similar text descriptions\")\n",
    "            gr.Markdown(\"*Upload an image to discover semantically related text descriptions from our dataset.*\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=3):\n",
    "                    image_input = gr.Image(\n",
    "                        label=\"Upload Image\", \n",
    "                        type=\"pil\",\n",
    "                        height=400\n",
    "                    )\n",
    "                with gr.Column(scale=1):\n",
    "                    image_num_results = gr.Slider(\n",
    "                        minimum=1,\n",
    "                        maximum=20,\n",
    "                        value=5,\n",
    "                        step=1,\n",
    "                        label=\"Results Count\"\n",
    "                    )\n",
    "            \n",
    "            image_btn = gr.Button(\"üîç Find Similar Text\", variant=\"primary\", size=\"lg\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                image_output = gr.Markdown(\n",
    "                    value=\"Upload an image above to find semantically similar text descriptions.\",\n",
    "                    label=\"Similar Text Descriptions\"\n",
    "                )\n",
    "    \n",
    "    # Event Bindings\n",
    "    text_btn.click(\n",
    "        fn=perform_text_to_image_search,\n",
    "        inputs=[text_input, num_results],\n",
    "        outputs=[image_gallery, result_details]\n",
    "    )\n",
    "    \n",
    "    # Allow Enter key to trigger search\n",
    "    text_input.submit(\n",
    "        fn=perform_text_to_image_search,\n",
    "        inputs=[text_input, num_results],\n",
    "        outputs=[image_gallery, result_details]\n",
    "    )\n",
    "    \n",
    "    image_btn.click(\n",
    "        fn=perform_image_to_text_search,\n",
    "        inputs=[image_input, image_num_results],\n",
    "        outputs=image_output\n",
    "    )\n",
    "\n",
    "# Launch the interface\n",
    "logger.info(\"Starting image search interface...\")\n",
    "multimodal_demo.launch(inline=True, share=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
