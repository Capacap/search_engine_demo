{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration Constants\n",
        "\n",
        "# Model configuration\n",
        "MODEL_NAME = \"openai/clip-vit-base-patch32\"\n",
        "EMBEDDING_BATCH_SIZE = 32\n",
        "TEXT_MAX_LENGTH = 77\n",
        "\n",
        "# Dataset configuration  \n",
        "DATASET_ID = \"adityajn105/flickr8k\"\n",
        "EMBEDDINGS_OUTPUT_FILE = \"../data/flickr8k_embeddings.h5\"\n",
        "\n",
        "# HDF5 storage configuration\n",
        "HDF5_COMPRESSION = 'gzip'\n",
        "HDF5_COMPRESSION_LEVEL = 6\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO: Logging initialized\n"
          ]
        }
      ],
      "source": [
        "# Logging setup\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import h5py\n",
        "import torch\n",
        "import logging\n",
        "import kagglehub\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from typing import Dict\n",
        "from datetime import datetime\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(levelname)s: %(message)s',\n",
        "    stream=sys.stdout,\n",
        "    force=True\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.info(\"Logging initialized\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.12), please consider upgrading to the latest version (0.3.13).\n",
            "INFO: Dataset downloaded to: /home/capacap/Projects/search_engine_demo/data/datasets/adityajn105/flickr8k/versions/1\n",
            "INFO: Images: /home/capacap/Projects/search_engine_demo/data/datasets/adityajn105/flickr8k/versions/1/Images\n",
            "INFO: Captions: /home/capacap/Projects/search_engine_demo/data/datasets/adityajn105/flickr8k/versions/1/captions.txt\n"
          ]
        }
      ],
      "source": [
        "# Download the dataset to local data folder\n",
        "\n",
        "# Set custom cache directory to project's data folder\n",
        "project_root = os.path.dirname(os.getcwd())\n",
        "data_folder = os.path.join(project_root, 'data')\n",
        "os.makedirs(data_folder, exist_ok=True)\n",
        "os.environ['KAGGLEHUB_CACHE'] = data_folder\n",
        "\n",
        "# Download and store paths\n",
        "dataset_path = kagglehub.dataset_download(DATASET_ID)\n",
        "images_path = os.path.join(dataset_path, 'Images')\n",
        "captions_path = os.path.join(dataset_path, 'captions.txt')\n",
        "\n",
        "logger.info(f\"Dataset downloaded to: {dataset_path}\")\n",
        "logger.info(f\"Images: {images_path}\")\n",
        "logger.info(f\"Captions: {captions_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO: Device: cuda\n",
            "INFO: GPU: NVIDIA GeForce RTX 3060 Ti\n",
            "INFO: VRAM: 8.2 GB\n",
            "INFO: Loading openai/clip-vit-base-patch32...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO: Loaded openai/clip-vit-base-patch32\n"
          ]
        }
      ],
      "source": [
        "# Set device, load model and processor\n",
        "\n",
        "def get_device():\n",
        "    # Get device based on cuda support\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    logger.info(f\"Device: {device}\")\n",
        "    if torch.cuda.is_available():\n",
        "        logger.info(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        logger.info(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "    return device\n",
        "\n",
        "def load_model(model_name, device):\n",
        "    # Download and load model and processor\n",
        "    logger.info(f\"Loading {model_name}...\")\n",
        "    model = CLIPModel.from_pretrained(model_name).to(device)\n",
        "    processor = CLIPProcessor.from_pretrained(model_name)\n",
        "    logger.info(f\"Loaded {model_name}\")\n",
        "\n",
        "    return model, processor\n",
        "\n",
        "# Get the device to use\n",
        "device = get_device()\n",
        "\n",
        "# Load model and processor\n",
        "model, processor= load_model(MODEL_NAME, device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Custom Dataset Class for Flickr8k\n",
        "\n",
        "class Flickr8kDataset(Dataset):\n",
        "    def __init__(self, captions_file: str, images_dir: str):\n",
        "        self.images_dir = images_dir\n",
        "        \n",
        "        # Load and parse image-caption pairs\n",
        "        self.data = self._load_data_from_file(captions_file)\n",
        "\n",
        "        \n",
        "    def _load_data_from_file(self, captions_file: str) -> pd.DataFrame:\n",
        "        try:\n",
        "            # Read the CSV file\n",
        "            df = pd.read_csv(captions_file)\n",
        "            \n",
        "            # Cleanup - Remove any rows with missing values\n",
        "            df = df.dropna()\n",
        "            \n",
        "            # Cleanup - Remove missing or invalid images\n",
        "            valid_entries = []\n",
        "            n_missing, n_invalid = 0, 0\n",
        "            for _, row in df.iterrows():\n",
        "                image_path = os.path.join(self.images_dir, row['image'])\n",
        "                \n",
        "                # Check if image file exists\n",
        "                if not os.path.exists(image_path):\n",
        "                    n_missing += 1\n",
        "                    logger.info(f\"Skipping missing image {row['image']}\")\n",
        "                    continue\n",
        "                \n",
        "                # Check if image file is valid\n",
        "                try:\n",
        "                    with Image.open(image_path) as img:\n",
        "                        img.verify()\n",
        "                    valid_entries.append(row)\n",
        "                except Exception as e:\n",
        "                    n_invalid += 1\n",
        "                    logger.info(f\"Skipping corrupted image {row['image']}: {e}\")\n",
        "\n",
        "            # Log cleanup results\n",
        "            if n_missing > 0:\n",
        "                logger.info(f\"Skipped {n_missing} missing images\")\n",
        "\n",
        "            if n_invalid > 0:\n",
        "                logger.info(f\"Skipped {n_invalid} invalid images\")\n",
        "\n",
        "            # Raise error if no valid entries were found\n",
        "            if not valid_entries:\n",
        "                raise ValueError(f\"No valid image-caption pairs found in {captions_file}: {e}\")\n",
        "                \n",
        "            logger.info(f\"Loaded {len(valid_entries)} image-caption pairs from {captions_file}\")\n",
        "\n",
        "            return pd.DataFrame(valid_entries)\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading captions file: {e}\")\n",
        "            raise\n",
        "    \n",
        "    def __len__(self) -> int:\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx: int) -> Dict:\n",
        "        # Find image-caption pair\n",
        "        row = self.data.iloc[idx]\n",
        "        image_name = row['image']\n",
        "        caption = row['caption']\n",
        "        \n",
        "        # Load image\n",
        "        image_path = os.path.join(self.images_dir, image_name)\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        \n",
        "        return {\n",
        "            'image': image,\n",
        "            'caption': caption,\n",
        "            'image_name': image_name,\n",
        "            'image_path': image_path\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function for processing of the dataset and into embeddings data\n",
        "\n",
        "def generate_embeddings(\n",
        "    dataset: Flickr8kDataset, \n",
        "    model: CLIPModel, \n",
        "    processor: CLIPProcessor, \n",
        "    device: str, \n",
        "    batch_size: int\n",
        "    ):\n",
        "    # Initialize embeddings data\n",
        "    embeddings_data = {\n",
        "        'image_embeddings': [],\n",
        "        'text_embeddings': [], \n",
        "        'image_names': [],\n",
        "        'captions': []\n",
        "    }\n",
        "    \n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Process the dataset in batches\n",
        "        for batch_start in tqdm(range(0, len(dataset), batch_size), desc=\"Generating embeddings\", unit=\"batch\"):\n",
        "            batch_end = min(batch_start + batch_size, len(dataset))\n",
        "            batch_images = []\n",
        "            batch_captions = []\n",
        "            batch_names = []\n",
        "\n",
        "            # Collect batch data\n",
        "            for i in range(batch_start, batch_end):\n",
        "                item = dataset[i]\n",
        "                batch_images.append(item[\"image\"])\n",
        "                batch_captions.append(item[\"caption\"])\n",
        "                batch_names.append(item[\"image_name\"])\n",
        "\n",
        "            # Process batch images on device\n",
        "            image_inputs = processor(\n",
        "                images=batch_images, \n",
        "                return_tensors=\"pt\", \n",
        "                padding=True\n",
        "            ).to(device)\n",
        "            image_embeddings = model.get_image_features(**image_inputs)\n",
        "\n",
        "            # Process batch captions on device\n",
        "            caption_inputs = processor(\n",
        "                text=batch_captions, \n",
        "                return_tensors=\"pt\", \n",
        "                padding=True, \n",
        "                truncation=True,\n",
        "                max_length=TEXT_MAX_LENGTH\n",
        "                ).to(device)\n",
        "            caption_embeddings = model.get_text_features(**caption_inputs)\n",
        "\n",
        "            # Normalize image embeddings\n",
        "            batch_img_embs = image_embeddings.cpu().numpy()\n",
        "            batch_img_norms = np.linalg.norm(batch_img_embs, axis=1, keepdims=True)\n",
        "            batch_img_normalized = batch_img_embs / batch_img_norms\n",
        "\n",
        "            # Normalize text embeddings\n",
        "            batch_txt_embs = caption_embeddings.cpu().numpy()\n",
        "            batch_txt_norms = np.linalg.norm(batch_txt_embs, axis=1, keepdims=True)\n",
        "            batch_txt_normalized = batch_txt_embs / batch_txt_norms\n",
        "\n",
        "            # Update embeddings data\n",
        "            embeddings_data[\"image_embeddings\"].extend(batch_img_normalized)\n",
        "            embeddings_data[\"text_embeddings\"].extend(batch_txt_normalized)\n",
        "            embeddings_data[\"image_names\"].extend(batch_names)\n",
        "            embeddings_data[\"captions\"].extend(batch_captions)\n",
        "\n",
        "            # Clean up cuda cache\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "    return embeddings_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to save embeddings to HDF5 file\n",
        "\n",
        "def save_embeddings_to_file(\n",
        "    embeddings_data: dict, \n",
        "    filepath: str, \n",
        "    model_name: str,\n",
        "    dataset_path: str,\n",
        "    images_path: str,\n",
        "    captions_path: str\n",
        "    ):\n",
        "    logger.info(f\"Saving {len(embeddings_data['image_embeddings'])} embeddings to {filepath}...\")\n",
        "    try:\n",
        "        # Ensure target directory exists\n",
        "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
        "\n",
        "        # Convert to numpy arrays\n",
        "        img_embs = np.array(embeddings_data['image_embeddings'], dtype=np.float32)\n",
        "        txt_embs = np.array(embeddings_data['text_embeddings'], dtype=np.float32)\n",
        "        \n",
        "        # Create the .h5 file\n",
        "        with h5py.File(filepath, 'w') as f:\n",
        "            # Store image embeddings\n",
        "            f.create_dataset('image_embeddings', data=img_embs, compression=HDF5_COMPRESSION, compression_opts=HDF5_COMPRESSION_LEVEL)\n",
        "            \n",
        "            # Store text embeddings\n",
        "            f.create_dataset('text_embeddings', data=txt_embs, compression=HDF5_COMPRESSION, compression_opts=HDF5_COMPRESSION_LEVEL)\n",
        "            \n",
        "            # Set data type for string data\n",
        "            dt = h5py.string_dtype(encoding='utf-8')\n",
        "\n",
        "            # Store image names\n",
        "            f.create_dataset('image_names', data=embeddings_data['image_names'], dtype=dt, compression=HDF5_COMPRESSION)\n",
        "            \n",
        "            # Store image captions\n",
        "            f.create_dataset('captions', data=embeddings_data['captions'], dtype=dt, compression=HDF5_COMPRESSION)\n",
        "            \n",
        "            # Set metadata\n",
        "            f.attrs['creation_date'] = datetime.now().isoformat()\n",
        "            f.attrs['total_samples'] = len(embeddings_data['image_embeddings'])\n",
        "            f.attrs['embedding_dim'] = img_embs.shape[1]\n",
        "            f.attrs['model_name'] = model_name\n",
        "            f.attrs['dataset_path'] = dataset_path\n",
        "            f.attrs['images_path'] = images_path\n",
        "            f.attrs['captions_path'] = captions_path\n",
        "            \n",
        "        logger.info(f\"Embeddings data saved to {filepath}\")\n",
        "        logger.info(f\"Images path stored: {images_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to save embeddings data: {e}\")\n",
        "        raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO: Loaded 40455 image-caption pairs from /home/capacap/Projects/search_engine_demo/data/datasets/adityajn105/flickr8k/versions/1/captions.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating embeddings: 100%|██████████| 1265/1265 [04:52<00:00,  4.33batch/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO: Saving 40455 embeddings to ../data/flickr8k_embeddings.h5...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO: Embeddings data saved to ../data/flickr8k_embeddings.h5\n",
            "INFO: Images path stored: /home/capacap/Projects/search_engine_demo/data/datasets/adityajn105/flickr8k/versions/1/Images\n",
            "File structure: ['captions', 'image_embeddings', 'image_names', 'text_embeddings']\n",
            "Metadata: {'captions_path': '/home/capacap/Projects/search_engine_demo/data/datasets/adityajn105/flickr8k/versions/1/captions.txt', 'creation_date': '2025-08-28T21:38:35.963176', 'dataset_path': '/home/capacap/Projects/search_engine_demo/data/datasets/adityajn105/flickr8k/versions/1', 'embedding_dim': np.int64(512), 'images_path': '/home/capacap/Projects/search_engine_demo/data/datasets/adityajn105/flickr8k/versions/1/Images', 'model_name': 'openai/clip-vit-base-patch32', 'total_samples': np.int64(40455)}\n"
          ]
        }
      ],
      "source": [
        "# Execute the main workflow\n",
        "\n",
        "# Load dataset\n",
        "dataset = Flickr8kDataset(captions_path, images_path)\n",
        "\n",
        "# Generate embeddings\n",
        "embeddings_data = generate_embeddings(\n",
        "    dataset=dataset,\n",
        "    model=model,\n",
        "    processor=processor,\n",
        "    device=device,\n",
        "    batch_size=EMBEDDING_BATCH_SIZE\n",
        ")\n",
        "\n",
        "# Save data to file\n",
        "save_embeddings_to_file(\n",
        "    embeddings_data=embeddings_data,\n",
        "    filepath=EMBEDDINGS_OUTPUT_FILE, \n",
        "    model_name=MODEL_NAME,\n",
        "    dataset_path=dataset_path,\n",
        "    images_path=images_path,\n",
        "    captions_path=captions_path\n",
        ")\n",
        "\n",
        "# Basic validation of saved data\n",
        "with h5py.File(EMBEDDINGS_OUTPUT_FILE, 'r') as f:\n",
        "    print(f\"File structure: {list(f.keys())}\")\n",
        "    print(f\"Metadata: {dict(f.attrs)}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
